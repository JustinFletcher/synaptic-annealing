\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {2.1.}{\ignorespaces (Bottom) A simple step potential in one dimension. (Top) The probability density function of a generic quantum system in the presence of a potential energy step barrier. There is an exponential decrease in probability through the barrier, and a uniform probability beyond the barrier.}}{2-15}
\addvspace {10pt}
\contentsline {figure}{\numberline {3.1.}{\ignorespaces (a) A potential energy barrier on a one-dimensional potential energy surface. (b) The tunneling probability relative to the probability of traversal due to thermal fluctuation for a step barrier plotted as a function of the height and width of the barrier.}}{3-4}
\contentsline {figure}{\numberline {3.2.}{\ignorespaces This figure comprises four plots, each of which displays a GSA distribution at various values of $T_{q_V}$, in order to illustrate the behavior of the GSA distribution for several values of $q_V$. This figure shows the GSA near the mean, which illustrates the effect of $q_v$ and $T_{q_V}$ on the near-mean domain values, while neglecting the effects on the tails of the distribution.}}{3-6}
\contentsline {figure}{\numberline {3.3.}{\ignorespaces This figure comprises four plots, each of which displays a GSA distribution at various values of $T_{q_V}$, in order to illustrate the behavior of the GSA distribution for several values of $q_V$. This figure shows the GSA distribution for domain values far from the mean, which illustrates the effect of $q_v$ and $T_{q_V}$ on the tails of the distribution.}}{3-8}
\contentsline {figure}{\numberline {3.4.}{\ignorespaces (a) An arbitrary feed-forward ANN. (b) The weight matrix representation of the feed-forward ANN in (a).}}{3-11}
\contentsline {figure}{\numberline {3.5.}{\ignorespaces (a) A plot showing the traversal of the $w_1,w_2$ subspace of the weight space over the course of $3,000$ epochs produced by a synaptic annealing algorithm employing a Gaussian visiting distribution. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-16}
\contentsline {figure}{\numberline {3.6.}{\ignorespaces (a) A plot showing the traversal of the $w_1,w_2$ subspace of the weight space over the course of $3,000$ epochs produced by a synaptic annealing algorithm employing a Cauchy visiting distribution. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-17}
\contentsline {figure}{\numberline {3.7.}{\ignorespaces (a) A plot showing the traversal of the $w_1,w_2$ subspace of the weight space over the course of $3,000$ epochs produced by a synaptic annealing algorithm employing an isotropic GSA visiting distribution. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-18}
\contentsline {figure}{\numberline {3.8.}{\ignorespaces (a) A plot showing the traversal of the $(w_1,w_2)$ subspace of the weight space over the course of $3,000$ epochs produced by a synaptic annealing algorithm employing a visiting distribution which is uniform over the rang $[-2,2]$. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-19}
\addvspace {10pt}
\contentsline {figure}{\numberline {4.1.}{\ignorespaces The simulation-time evolution of the 25-fold cross-validated classification error of a feed-forward neural network with 50 hidden units on Fishers iris data. Each column in this table depicts the results for an SA neighborhood function employing a single type of visiting distribution, which is indicated in the column header. Each row depicts a single type of anisotropicity strategy, which is indicated in the row header. The classification error is defined as the fraction of samples incorrectly classified.}}{4-1}
\contentsline {figure}{\numberline {4.2.}{\ignorespaces The simulation-time evolution of the fraction of 25 independent simulated annealing runs achieving perfect classification of Fishers iris data using a feed-forward neural network with 50 hidden units. Columns and rows are labeled as in Fig.\nobreakspace {}4.1\hbox {}.}}{4-4}
\contentsline {figure}{\numberline {4.3.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all Gaussian visiting distributions, and thus corresponds to the first column of Fig.\nobreakspace {}4.0.1.2\hbox {}}}{4-5}
\contentsline {figure}{\numberline {4.4.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all exponential visiting distributions, and thus corresponds to the second column of Fig.\nobreakspace {}4.0.1.2\hbox {}}}{4-6}
\contentsline {figure}{\numberline {4.5.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all uniform visiting distributions, and thus corresponds to the third column of Fig.\nobreakspace {}4.0.1.2\hbox {}}}{4-6}
\contentsline {figure}{\numberline {4.6.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all Cauchy visiting distributions, and thus corresponds to the fourth column of Fig.\nobreakspace {}4.0.1.2\hbox {}}}{4-7}
\addvspace {10pt}
