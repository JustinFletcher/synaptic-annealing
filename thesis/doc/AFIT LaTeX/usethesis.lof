\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {2.1.}{\ignorespaces A simple perceptron.}}{2-4}
\contentsline {figure}{\numberline {2.2.}{\ignorespaces (Bottom) A simple step potential in one dimension. (Top) The probability density function of a generic quantum system in the presence of a potential energy step barrier. There is an exponential decrease in probability through the barrier, and a uniform probability beyond the barrier.}}{2-15}
\addvspace {10pt}
\contentsline {figure}{\numberline {3.1.}{\ignorespaces (a) A potential energy barrier on a one-dimensional potential energy surface. (b) The tunneling probability relative to the probability of traversal due to thermal fluctuation for a step barrier plotted as a function of the height and width of the barrier.}}{3-4}
\contentsline {figure}{\numberline {3.2.}{\ignorespaces This figure comprises four plots, each of which displays a GSA distribution at various values of $T_{q_V}$, in order to illustrate the behavior of the GSA distribution for several values of $q_V$. This figure shows the GSA near the mean, which illustrates the effect of $q_v$ and $T_{q_V}$ on the near-mean domain values, while neglecting the effects on the tails of the distribution.}}{3-6}
\contentsline {figure}{\numberline {3.3.}{\ignorespaces This figure comprises four plots, each of which displays a GSA distribution at various values of $T_{q_V}$, in order to illustrate the behavior of the GSA distribution for several values of $q_V$. This figure shows the GSA distribution for domain values far from the mean, which illustrates the effect of $q_v$ and $T_{q_V}$ on the tails of the distribution.}}{3-8}
\contentsline {figure}{\numberline {3.4.}{\ignorespaces (a) An arbitrary feed-forward ANN. (b) The weight matrix representation of the feed-forward ANN in (a).}}{3-11}
\contentsline {figure}{\numberline {3.5.}{\ignorespaces (a) A plot showing the traversal of the $w_1,w_2$ subspace of the weight space over the course of $5,000$ epochs produced by a synaptic annealing algorithm employing a Gaussian visiting distribution. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch, smoothed using a central moving window average with a width of $21$. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-17}
\contentsline {figure}{\numberline {3.6.}{\ignorespaces (a) A plot showing the traversal of the $w_1,w_2$ subspace of the weight space over the course of $5,000$ epochs produced by a synaptic annealing algorithm employing a Cauchy visiting distribution. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch, smoothed using a central moving window average with a width of $21$. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-18}
\contentsline {figure}{\numberline {3.7.}{\ignorespaces (a) A plot showing the traversal of the $w_1,w_2$ subspace of the weight space over the course of $5,000$ epochs produced by a synaptic annealing algorithm employing an isotropic GSA visiting distribution. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch, smoothed using a central moving window average with a width of $21$. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-20}
\contentsline {figure}{\numberline {3.8.}{\ignorespaces (a) A plot showing the traversal of the $w_1,w_2$ subspace of the weight space over the course of $5,000$ epochs produced by a synaptic annealing algorithm employing a GSA visiting distribution with synaptic weight-based anisotropicity. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch, smoothed using a central moving window average with a width of $21$. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-21}
\contentsline {figure}{\numberline {3.9.}{\ignorespaces (a) A plot showing the traversal of the $(w_1,w_2)$ subspace of the weight space over the course of $5,000$ epochs produced by a synaptic annealing algorithm employing a visiting distribution which is uniform over the range $\left [-\frac {1}{2},\frac {1}{2}\right ]$. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch, smoothed using a central moving window average with a width of $21$. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-23}
\contentsline {figure}{\numberline {3.10.}{\ignorespaces (a) A plot juxtaposing the weight space traversals of produced by a synaptic annealing algorithm employing several different visiting distributions.}}{3-24}
\contentsline {figure}{\numberline {3.11.}{\ignorespaces (a) A plot displaying the surface produced by the complicated interaction function, $f_{CI}$, in two dimensions, $u_1$ and $u_2$ (b) A color contour plot of the complicated interaction function.}}{3-27}
\addvspace {10pt}
\contentsline {figure}{\numberline {4.1.}{\ignorespaces The simulation-time evolution of the 25-fold cross-validated classification error of a feed-forward neural network with 50 hidden units on Fishers iris data. Each column in this table depicts the results for an SA neighborhood function employing a single type of visiting distribution, which is indicated in the column header. Each row depicts a single type of anisotropicity strategy, which is indicated in the row header. The classification error is defined as the fraction of samples incorrectly classified.}}{4-1}
\addvspace {10pt}
\addvspace {10pt}
