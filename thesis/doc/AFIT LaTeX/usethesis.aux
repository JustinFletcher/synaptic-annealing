\relax 
\@writefile{toc}{\contentsline {chapter}{Preface}{iii}}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{vi}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{x}}
\@writefile{toc}{\contentsline {chapter}{List of Symbols}{xi}}
\@writefile{toc}{\contentsline {chapter}{List of Abbreviations}{xii}}
\@writefile{loa}{\contentsline {figure}{ANN\ Artificial Neural Network}{xii}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{xiii}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {I.}Introduction}{1-1}}
\citation{haykin1999}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {II.}Background}{2-1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Artificial Neural Networks}{2-1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Biological Inspiration}{2-1}}
\citation{mcculloch1988}
\citation{piccinini2006}
\citation{mcculloch1988}
\citation{mcculloch1988}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Historical Overview}{2-2}}
\citation{rochester1956}
\citation{james1890principles}
\citation{hebb1967}
\citation{rosenblatt1958perceptron}
\citation{anderson1988neurocomputing}
\citation{block1962perceptron}
\citation{widrow1960asc}
\citation{minsky1969perceptrons}
\citation{anderson1988neurocomputing}
\citation{anderson1972interactive}
\citation{kohonen1972correlation}
\citation{vonderMalsburg1973selforganization}
\citation{anderson1988neurocomputing}
\citation{brodie78limulus}
\citation{anderson1988neurocomputing}
\citation{hopfield1982neuralnetworks}
\newlabel{eq:boltzmannProb}{{2.5}{2-8}}
\citation{anderson1988neurocomputing}
\citation{barto1983neuronlike}
\citation{haykin1999}
\newlabel{eq:boltzmannDist}{{2.6}{2-9}}
\citation{bryson1969optimalcontrol}
\citation{werbos1974beyondregression}
\citation{parker1985logic}
\citation{lecun1985}
\citation{rumelhart1986internal}
\citation{widrow1960asc}
\citation{minsky1969perceptrons}
\citation{cybenko1989approxsuperposition}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Network Topology}{2-10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Activation Functions}{2-10}}
\citation{engel1988teachingfeedforwardnueralnetsbysa}
\citation{kirkpatrick1983}
\citation{metropolis1953}
\citation{goffe1994globaloptimization}
\citation{corana1987minimizingmultimodal}
\citation{lecchinivisintini2007sacontinuousgaruntees}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Learning Strategies}{2-11}}
\newlabel{sec:backpropTraining}{{2.1.5.1}{2-11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5.1}Back Propagation Training}{2-11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5.2}Simulated Annealing Training}{2-11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Related Works in Simulated Annealing}{2-11}}
\newlabel{eq:simulatedAnnealing}{{2.2}{2-11}}
\newlabel{eq:cost_mapping}{{2.2}{2-11}}
\citation{kirkpatrick1983}
\citation{metropolis1953}
\newlabel{eq:delta_cost}{{2.2}{2-12}}
\newlabel{eq:thermal_traversal_prob}{{2.8}{2-12}}
\citation{szu1987fastsimulatedannealing}
\citation{tsallis1996generalizedsimulatedannealing}
\citation{ingber1989veryfastsimulatedreannealing}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Simulated Annealing}}{2-13}}
\newlabel{alg:simulated_annealing}{{1}{2-13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Reannealing}{2-13}}
\citation{mukherjee2015multivariatesearchqa}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Related Works in Quantum Mechanics}{2-14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Quantum Tunneling}{2-14}}
\newlabel{eq:quantum_tunneling_prob}{{2.9}{2-14}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Notation and Terminology Conventions}{2-14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1.}{\ignorespaces (Bottom) A simple step potential in one dimension. (Top) The probability density function of a generic quantum system in the presence of a potential energy step barrier. There is an exponential decrease in probability through the barrier, and a uniform probability beyond the barrier.}}{2-15}}
\newlabel{fig:quantum_tunneling}{{2.1}{2-15}}
\citation{szu1987fastsimulatedannealing}
\citation{tsallis1996generalizedsimulatedannealing}
\citation{ingber1989veryfastsimulatedreannealing}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {III.}Methodology}{3-1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Simulated Annealing}{3-1}}
\newlabel{scn:simulated_annealing}{{3.1}{3-1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Neighborhood Functions: Traversing the Cost Surface}{3-1}}
\citation{tsallis1996generalizedsimulatedannealing}
\citation{tsallis1996generalizedsimulatedannealing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Visiting Distributions}{3-2}}
\newlabel{scn:visiting_distributions}{{3.2.1}{3-2}}
\newlabel{scn:classical_visiting}{{3.2.1.1}{3-2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.1}Gaussian Visiting Distributions}{3-2}}
\citation{szu1987fastsimulatedannealing}
\newlabel{eq:normal_pdf}{{3.1}{3-3}}
\newlabel{visiting_distribution_abstract_cauchy}{{3.2.1.2}{3-3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.2}Cauchy Visiting Distributions}{3-3}}
\citation{tsallis1996generalizedsimulatedannealing}
\citation{tsallis1996generalizedsimulatedannealing}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1.}{\ignorespaces  (a) A potential energy barrier on a one-dimensional potential energy surface. (b) The tunneling probability relative to the probability of traversal due to thermal fluctuation for a step barrier plotted as a function of the height and width of the barrier.}}{3-4}}
\newlabel{fig:quantum_advantage}{{3.1}{3-4}}
\newlabel{eq:cauchy_pdf}{{3.2}{3-4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.3}Generalized Simulated Annealing Visiting Distribution}{3-5}}
\newlabel{eq:gsa_visiting_distribution}{{3.3}{3-5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2.}{\ignorespaces This figure comprises four plots, each of which displays a GSA distribution at various values of $T_{q_V}$, in order to illustrate the behavior of the GSA distribution for several values of $q_V$. This figure shows the GSA near the mean, which illustrates the effect of $q_v$ and $T_{q_V}$ on the near-mean domain values, while neglecting the effects on the tails of the distribution.}}{3-6}}
\newlabel{fig:gsa_distribution}{{3.2}{3-6}}
\citation{dallinga2004performancegsa}
\citation{andricioaei1996applicationgsatetrapide}
\citation{dallinga2004performancegsa}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3.}{\ignorespaces This figure comprises four plots, each of which displays a GSA distribution at various values of $T_{q_V}$, in order to illustrate the behavior of the GSA distribution for several values of $q_V$. This figure shows the GSA distribution for domain values far from the mean, which illustrates the effect of $q_v$ and $T_{q_V}$ on the tails of the distribution.}}{3-8}}
\newlabel{fig:gsa_distribution_wing_detail}{{3.3}{3-8}}
\citation{tsallis1996generalizedsimulatedannealing}
\citation{dallinga2004performancegsa}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.4}Uniform Visiting Distributions}{3-9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Anisotropicity Policies}{3-9}}
\citation{lee2007improvinggeneralizationcapabilitynnusingsa}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.1}Isotropic Anisotropicity}{3-10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.2}State-Based Anisotropicity}{3-10}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Feed-Forward Neural Network Representation}{3-10}}
\newlabel{scn:feed_forward_neural_network_representation}{{3.3}{3-10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4.}{\ignorespaces  (a) An arbitrary feed-forward ANN. (b) The weight matrix representation of the feed-forward ANN in (a).}}{3-11}}
\newlabel{fig:nettomatmapping}{{3.4}{3-11}}
\newlabel{fig:abstract_matrix_rep}{{3.3}{3-11}}
\newlabel{eq:cost_mapping_ffnn}{{3.3}{3-11}}
\newlabel{eq:optimal_definition}{{3.3}{3-12}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Applying Simulated Annealing to Feed-Forward Neural Network Weight Modification}{3-12}}
\newlabel{scn:classical_neighborhood}{{3.4.1}{3-12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Synaptic Annealing Neighborhood Functions}{3-12}}
\newlabel{eq:generic_anisotropic_neighborhood}{{3.4}{3-12}}
\citation{kirkpatrick1983}
\newlabel{eq:perturbation_temp_matrix_constraints}{{3.5}{3-13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.1}Gaussian (CSA) Synaptic Annealing Neighborhood}{3-13}}
\newlabel{eq:gaussian_neighborhood}{{3.6}{3-13}}
\citation{lee2007improvinggeneralizationcapabilitynnusingsa}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.2}Cauchy (FSA) Synaptic Annealing Neighborhood}{3-14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.3}Tsallis (GSA) Synaptic Annealing Neighborhood}{3-14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.4}Uniform Synaptic Annealing Neighborhood}{3-14}}
\newlabel{scn:ffnn_anisotropicity}{{3.4.2}{3-14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Feed-Forward Neural Network Anisotropicity Policies}{3-14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Weight Space Traversal}{3-14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Gaussian Neighborhood Function Weight Space Traversal}{3-15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Cauchy Neighborhood Function Weight Space Traversal}{3-15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}GSA Neighborhood Function Weight Space Traversal}{3-15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Uniform Neighborhood Function Weight Space Traversal}{3-15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5.}{\ignorespaces  (a) A plot showing the traversal of the $w_1,w_2$ subspace of the weight space over the course of $3,000$ epochs produced by a synaptic annealing algorithm employing a Gaussian visiting distribution. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-16}}
\newlabel{fig:weight_space_gaussian}{{3.5}{3-16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Cost Functions}{3-16}}
\newlabel{scn:cost_functions}{{3.6}{3-16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Design of Experiments}{3-16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6.}{\ignorespaces  (a) A plot showing the traversal of the $w_1,w_2$ subspace of the weight space over the course of $3,000$ epochs produced by a synaptic annealing algorithm employing a Cauchy visiting distribution. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-17}}
\newlabel{fig:weight_space_cauchy}{{3.6}{3-17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7.}{\ignorespaces  (a) A plot showing the traversal of the $w_1,w_2$ subspace of the weight space over the course of $3,000$ epochs produced by a synaptic annealing algorithm employing an isotropic GSA visiting distribution. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-18}}
\newlabel{fig:weight_space_gsa_i}{{3.7}{3-18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8.}{\ignorespaces (a) A plot showing the traversal of the $(w_1,w_2)$ subspace of the weight space over the course of $3,000$ epochs produced by a synaptic annealing algorithm employing a visiting distribution which is uniform over the rang $[-2,2]$. A solid line indicates a move which was accepted by the simulate annealing algorithm, while a dashed line indicates a move which was rejected. In this figure, only a few rejected moves are visible. The word $start$ indicates the initial value of $(w_1,w_2)$, while the word $end$ denotes the final value. (b) (upper) A plot showing the both the training and validation MSE of the results produced by the neural network in each epoch. This is the post-perturbation error, meaning that the error associated with moves that were rejected is shown. (lower) A plot showing the sum of squared weights of the neural network during each training epoch.}}{3-19}}
\newlabel{fig:weight_space_uniform}{{3.8}{3-19}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {IV.}Results}{4-1}}
\newlabel{ch:results}{{IV}{4-1}}
\newlabel{block1a}{{1a}{4-1}}
\newlabel{block1b}{{1b}{4-1}}
\newlabel{block1c}{{1c}{4-1}}
\newlabel{block1d}{{1d}{4-1}}
\newlabel{block2a}{{2a}{4-1}}
\newlabel{block1b}{{2b}{4-1}}
\newlabel{block1b}{{2c}{4-1}}
\newlabel{block2b}{{2d}{4-1}}
\newlabel{block3a}{{3a}{4-1}}
\newlabel{block1b}{{3b}{4-1}}
\newlabel{block1b}{{3c}{4-1}}
\newlabel{block3b}{{3d}{4-1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1.}{\ignorespaces The simulation-time evolution of the 25-fold cross-validated classification error of a feed-forward neural network with 50 hidden units on Fishers iris data. Each column in this table depicts the results for an SA neighborhood function employing a single type of visiting distribution, which is indicated in the column header. Each row depicts a single type of anisotropicity strategy, which is indicated in the row header. The classification error is defined as the fraction of samples incorrectly classified.}}{4-1}}
\newlabel{fig:class_perf}{{4.1}{4-1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}Classification Performance}{4-1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.1.1}Visiting Distribution Analysis}{4-2}}
\newlabel{scn:anisotropicity_analysis}{{4.0.1.2}{4-2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.1.2}Anisotropicity Analysis}{4-2}}
\citation{ingber1989veryfastsimulatedreannealing}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.1.3}Perfect Classification Rate Analysis}{4-3}}
\newlabel{fig:perfect_class_time}{{4.0.1.2}{4-4}}
\newlabel{block1a}{{1a}{4-4}}
\newlabel{block1b}{{1b}{4-4}}
\newlabel{block1c}{{1c}{4-4}}
\newlabel{block1d}{{1d}{4-4}}
\newlabel{block2a}{{2a}{4-4}}
\newlabel{block1b}{{2b}{4-4}}
\newlabel{block1b}{{2c}{4-4}}
\newlabel{block2b}{{2d}{4-4}}
\newlabel{block3a}{{3a}{4-4}}
\newlabel{block1b}{{3b}{4-4}}
\newlabel{block1b}{{3c}{4-4}}
\newlabel{block3b}{{3d}{4-4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2.}{\ignorespaces The simulation-time evolution of the fraction of 25 independent simulated annealing runs achieving perfect classification of Fishers iris data using a feed-forward neural network with 50 hidden units. Columns and rows are labeled as in Fig.\nobreakspace  {}4.1\hbox {}.}}{4-4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all Gaussian visiting distributions, and thus corresponds to the first column of Fig.\nobreakspace  {}4.0.1.2\hbox {}}}{4-5}}
\newlabel{fig:gaussian_gamma}{{4.3}{4-5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all exponential visiting distributions, and thus corresponds to the second column of Fig.\nobreakspace  {}4.0.1.2\hbox {}}}{4-6}}
\newlabel{fig:exp_gamma}{{4.4}{4-6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all uniform visiting distributions, and thus corresponds to the third column of Fig.\nobreakspace  {}4.0.1.2\hbox {}}}{4-6}}
\newlabel{fig:uniform_gamma}{{4.5}{4-6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all Cauchy visiting distributions, and thus corresponds to the fourth column of Fig.\nobreakspace  {}4.0.1.2\hbox {}}}{4-7}}
\newlabel{fig:cauchy_gamma}{{4.6}{4-7}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Conclusion}{4-7}}
\newlabel{scn:conclusion}{{4.1}{4-7}}
\bibstyle{thesnumb}
\bibdata{synapticAnnealingBib}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\numberline {Appendix A.}First appendix title}{A-1}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}In an appendix}{A-1}}
\bibcite{anderson1972interactive}{1}
\bibcite{anderson1988neurocomputing}{2}
\bibcite{andricioaei1996applicationgsatetrapide}{3}
\bibcite{barto1983neuronlike}{4}
\bibcite{block1962perceptron}{5}
\bibcite{brodie78limulus}{6}
\bibcite{bryson1969optimalcontrol}{7}
\bibcite{corana1987minimizingmultimodal}{8}
\bibcite{cybenko1989approxsuperposition}{9}
\bibcite{dallinga2004performancegsa}{10}
\bibcite{engel1988teachingfeedforwardnueralnetsbysa}{11}
\bibcite{goffe1994globaloptimization}{12}
\bibcite{haykin1999}{13}
\bibcite{hebb1967}{14}
\bibcite{hopfield1982neuralnetworks}{15}
\bibcite{ingber1989veryfastsimulatedreannealing}{16}
\bibcite{james1890principles}{17}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{BIB-1}}
\bibcite{kirkpatrick1983}{18}
\bibcite{kohonen1972correlation}{19}
\bibcite{lecchinivisintini2007sacontinuousgaruntees}{20}
\bibcite{lecun1985}{21}
\bibcite{lee2007improvinggeneralizationcapabilitynnusingsa}{22}
\bibcite{mcculloch1988}{23}
\bibcite{metropolis1953}{24}
\bibcite{minsky1969perceptrons}{25}
\bibcite{mukherjee2015multivariatesearchqa}{26}
\bibcite{parker1985logic}{27}
\bibcite{piccinini2006}{28}
\bibcite{rochester1956}{29}
\bibcite{rosenblatt1958perceptron}{30}
\bibcite{rumelhart1986internal}{31}
\bibcite{szu1987fastsimulatedannealing}{32}
\bibcite{tsallis1996generalizedsimulatedannealing}{33}
\bibcite{vonderMalsburg1973selforganization}{34}
\bibcite{werbos1974beyondregression}{35}
\bibcite{widrow1960asc}{36}
\@writefile{toc}{\contentsline {chapter}{Vita}{VITA-1}}
