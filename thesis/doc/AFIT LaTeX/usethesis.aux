\relax 
\@writefile{toc}{\contentsline {chapter}{Preface}{iii}}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{vi}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{viii}}
\@writefile{toc}{\contentsline {chapter}{List of Symbols}{ix}}
\@writefile{toc}{\contentsline {chapter}{List of Abbreviations}{x}}
\@writefile{loa}{\contentsline {figure}{ANN\ Artificial Neural Network}{x}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{xi}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {I.}Introduction}{1-1}}
\citation{haykin1999}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {II.}Background}{2-1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Artificial Neural Networks}{2-1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Biological Inspiration}{2-1}}
\citation{mcculloch1988}
\citation{piccinini2006}
\citation{mcculloch1988}
\citation{mcculloch1988}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Historical Overview}{2-2}}
\citation{rochester1956}
\citation{james1890principles}
\citation{hebb1967}
\citation{rosenblatt1958perceptron}
\citation{anderson1988neurocomputing}
\citation{block1962perceptron}
\citation{widrow1960asc}
\citation{minsky1969perceptrons}
\citation{anderson1988neurocomputing}
\citation{anderson1972interactive}
\citation{kohonen1972correlation}
\citation{vonderMalsburg1973selforganization}
\citation{anderson1988neurocomputing}
\citation{brodie78limulus}
\citation{anderson1988neurocomputing}
\citation{hopfield1982neuralnetworks}
\newlabel{eq:boltzmannProb}{{2.5}{2-8}}
\citation{anderson1988neurocomputing}
\citation{barto1983neuronlike}
\citation{haykin1999}
\newlabel{eq:boltzmannDist}{{2.6}{2-9}}
\citation{bryson1969optimalcontrol}
\citation{werbos1974beyondregression}
\citation{parker1985logic}
\citation{lecun1985}
\citation{rumelhart1986internal}
\citation{widrow1960asc}
\citation{minsky1969perceptrons}
\citation{cybenko1989approxsuperposition}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Network Topology}{2-10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Activation Functions}{2-10}}
\citation{kirkpatrick1983}
\citation{metropolis1953}
\citation{goffe1994globaloptimization}
\citation{corana1987minimizingmultimodal}
\citation{lecchinivisintini2007sacontinuousgaruntees}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Learning Strategies}{2-11}}
\newlabel{sec:backpropTraining}{{2.1.5.1}{2-11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5.1}Back Propagation Training}{2-11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5.2}Simulated Annealing Training}{2-11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Related Works in Simulated Annealing}{2-11}}
\newlabel{eq:simulatedAnnealing}{{2.2}{2-11}}
\newlabel{eq:cost_mapping}{{2.2}{2-11}}
\citation{kirkpatrick1983}
\citation{metropolis1953}
\newlabel{eq:delta_cost}{{2.2}{2-12}}
\newlabel{eq:thermal_traversal_prob}{{2.8}{2-12}}
\citation{szu1987fastsimulatedannealing}
\citation{tsallis1996generalizedsimulatedannealing}
\citation{ingber1989veryfastsimulatedreannealing}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Simulated Annealing}}{2-13}}
\newlabel{alg:simulated_annealing}{{1}{2-13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Reannealing}{2-13}}
\citation{mukherjee2015multivariatesearchqa}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Related Works in Quantum Mechanics}{2-14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Quantum Tunneling}{2-14}}
\newlabel{eq:quantum_tunneling_prob}{{2.9}{2-14}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Notation and Terminology Conventions}{2-14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1.}{\ignorespaces (Bottom) A simple step potential in one dimension. (Top) The probability density function of a generic quantum system in the presence of a potential energy step barrier. There is an exponential decrease in probability through the barrier, and a uniform probability beyond the barrier.}}{2-15}}
\newlabel{fig:quantum_tunneling}{{2.1}{2-15}}
\citation{szu1987fastsimulatedannealing}
\citation{tsallis1996generalizedsimulatedannealing}
\citation{ingber1989veryfastsimulatedreannealing}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {III.}Methodology}{3-1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Simulated Annealing}{3-1}}
\newlabel{scn:simulated_annealing}{{3.1}{3-1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Neighborhood Functions: Traversing the Cost Surface}{3-1}}
\citation{tsallis1996generalizedsimulatedannealing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Visiting Distributions}{3-2}}
\newlabel{scn:visiting_distributions}{{3.2.1}{3-2}}
\newlabel{scn:classical_visiting}{{3.2.1.1}{3-2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.1}Gaussian Visiting Distributions}{3-2}}
\newlabel{visiting_distribution_abstract_cauchy}{{3.2.1.2}{3-2}}
\citation{szu1987fastsimulatedannealing}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.2}Cauchy Visiting Distributions}{3-3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1.}{\ignorespaces  (a) A potential energy barrier on a one-dimensional potential energy surface. (b) The tunneling probability relative to the probability of traversal due to thermal fluctuation for a step barrier plotted as a function of the height and width of the barrier.}}{3-3}}
\newlabel{fig:quantum_advantage}{{3.1}{3-3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.3}Exponential Visiting Distributions}{3-4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.4}Uniform Visiting Distributions}{3-4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2.}{\ignorespaces  (a) An arbitrary feed-forward ANN. (b) The weight matrix representation of the feed-forward ANN in (a).}}{3-5}}
\newlabel{fig:nettomatmapping}{{3.2}{3-5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Anisotropicity Policies}{3-5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.1}Isotropic Anisotropicity}{3-5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.2}Uniform Anisotropicity}{3-5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.3}Variable Anisotropicity}{3-5}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Feed-Forward Neural Network Representation}{3-5}}
\newlabel{scn:feed_forward_neural_network_representation}{{3.3}{3-5}}
\newlabel{fig:abstract_matrix_rep}{{3.3}{3-6}}
\newlabel{eq:cost_mapping_ffnn}{{3.3}{3-6}}
\newlabel{eq:optimal_definition}{{3.3}{3-6}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Applying SA to Feed-Forward Neural Networks}{3-7}}
\newlabel{scn:classical_neighborhood}{{3.4.1}{3-7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Classical Neighborhood Functions}{3-7}}
\newlabel{eq:classical_anisotropic_neighborhood}{{3.1}{3-7}}
\citation{tsallis1996generalizedsimulatedannealing}
\newlabel{scn:quantum_neighborhood}{{3.4.2}{3-8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Quantum-Inspired Neighborhood Functions}{3-8}}
\newlabel{eq:quantum_anisotropic_neighborhood}{{3.2}{3-8}}
\citation{mukherjee2015multivariatesearchqa}
\citation{szu1987fastsimulatedannealing}
\newlabel{eq:exp_generation_fcn}{{3.3}{3-9}}
\newlabel{eq:cauchy_generation_fcn}{{3.4}{3-9}}
\newlabel{eq:uniform_generation_fcn}{{3.5}{3-10}}
\newlabel{scn:ffnn_anisotropicity}{{3.4.2.1}{3-10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.1}Feed-Forward Neural Network Anisotropicity Policies}{3-10}}
\newlabel{eq:isotropic_anisotropicity}{{3.6}{3-11}}
\newlabel{eq:isotropic_anisotropicity_scaled}{{3.7}{3-11}}
\newlabel{eq:uniform_anisotropicity}{{3.9}{3-12}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Design of Experiments}{3-13}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {IV.}Results}{4-1}}
\newlabel{block1a}{{1a}{4-1}}
\newlabel{block1b}{{1b}{4-1}}
\newlabel{block1c}{{1c}{4-1}}
\newlabel{block1d}{{1d}{4-1}}
\newlabel{block2a}{{2a}{4-1}}
\newlabel{block1b}{{2b}{4-1}}
\newlabel{block1b}{{2c}{4-1}}
\newlabel{block2b}{{2d}{4-1}}
\newlabel{block3a}{{3a}{4-1}}
\newlabel{block1b}{{3b}{4-1}}
\newlabel{block1b}{{3c}{4-1}}
\newlabel{block3b}{{3d}{4-1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1.}{\ignorespaces The simulation-time evolution of the 25-fold cross-validated classification error of a feed-forward neural network with 50 hidden units on Fishers iris data. Each column in this table depicts the results for an SA neighborhood function employing a single type of visiting distribution, which is indicated in the column header. Each row depicts a single type of anisotropicity strategy, which is indicated in the row header. The classification error is defined as the fraction of samples incorrectly classified.}}{4-1}}
\newlabel{fig:class_perf}{{4.1}{4-1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}Classification Performance}{4-1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.1.1}Visiting Distribution Analysis}{4-2}}
\newlabel{scn:anisotropicity_analysis}{{4.0.1.2}{4-2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.1.2}Anisotropicity Analysis}{4-2}}
\citation{ingber1989veryfastsimulatedreannealing}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.0.1.3}Perfect Classification Rate Analysis}{4-3}}
\newlabel{fig:perfect_class_time}{{4.0.1.2}{4-4}}
\newlabel{block1a}{{1a}{4-4}}
\newlabel{block1b}{{1b}{4-4}}
\newlabel{block1c}{{1c}{4-4}}
\newlabel{block1d}{{1d}{4-4}}
\newlabel{block2a}{{2a}{4-4}}
\newlabel{block1b}{{2b}{4-4}}
\newlabel{block1b}{{2c}{4-4}}
\newlabel{block2b}{{2d}{4-4}}
\newlabel{block3a}{{3a}{4-4}}
\newlabel{block1b}{{3b}{4-4}}
\newlabel{block1b}{{3c}{4-4}}
\newlabel{block3b}{{3d}{4-4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2.}{\ignorespaces The simulation-time evolution of the fraction of 25 independent simulated annealing runs achieving perfect classification of Fishers iris data using a feed-forward neural network with 50 hidden units. Columns and rows are labeld as in Fig.\nobreakspace  {}4.1\hbox {}.}}{4-4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all Gaussian visiting distributions, and thus corresponds to the first column of Fig.\nobreakspace  {}4.0.1.2\hbox {}}}{4-5}}
\newlabel{fig:gaussian_gamma}{{4.3}{4-5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all exponential visiting distributions, and thus corresponds to the second column of Fig.\nobreakspace  {}4.0.1.2\hbox {}}}{4-6}}
\newlabel{fig:exp_gamma}{{4.4}{4-6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all uniform visiting distributions, and thus corresponds to the third column of Fig.\nobreakspace  {}4.0.1.2\hbox {}}}{4-6}}
\newlabel{fig:uniform_gamma}{{4.5}{4-6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6.}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all Cauchy visiting distributions, and thus corresponds to the fourth column of Fig.\nobreakspace  {}4.0.1.2\hbox {}}}{4-7}}
\newlabel{fig:cauchy_gamma}{{4.6}{4-7}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Conclusion}{4-7}}
\newlabel{scn:conclusion}{{4.1}{4-7}}
\bibstyle{thesnumb}
\bibdata{synapticAnnealingBib}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\numberline {Appendix A.}First appendix title}{A-1}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}In an appendix}{A-1}}
\bibcite{anderson1972interactive}{1}
\bibcite{anderson1988neurocomputing}{2}
\bibcite{barto1983neuronlike}{3}
\bibcite{block1962perceptron}{4}
\bibcite{brodie78limulus}{5}
\bibcite{bryson1969optimalcontrol}{6}
\bibcite{corana1987minimizingmultimodal}{7}
\bibcite{cybenko1989approxsuperposition}{8}
\bibcite{goffe1994globaloptimization}{9}
\bibcite{haykin1999}{10}
\bibcite{hebb1967}{11}
\bibcite{hopfield1982neuralnetworks}{12}
\bibcite{ingber1989veryfastsimulatedreannealing}{13}
\bibcite{james1890principles}{14}
\bibcite{kirkpatrick1983}{15}
\bibcite{kohonen1972correlation}{16}
\bibcite{lecchinivisintini2007sacontinuousgaruntees}{17}
\bibcite{lecun1985}{18}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{BIB-1}}
\bibcite{mcculloch1988}{19}
\bibcite{metropolis1953}{20}
\bibcite{minsky1969perceptrons}{21}
\bibcite{mukherjee2015multivariatesearchqa}{22}
\bibcite{parker1985logic}{23}
\bibcite{piccinini2006}{24}
\bibcite{rochester1956}{25}
\bibcite{rosenblatt1958perceptron}{26}
\bibcite{rumelhart1986internal}{27}
\bibcite{szu1987fastsimulatedannealing}{28}
\bibcite{tsallis1996generalizedsimulatedannealing}{29}
\bibcite{vonderMalsburg1973selforganization}{30}
\bibcite{werbos1974beyondregression}{31}
\bibcite{widrow1960asc}{32}
\@writefile{toc}{\contentsline {chapter}{Vita}{VITA-1}}
