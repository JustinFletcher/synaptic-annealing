\relax 
\citation{geman1984stochasticrelaxation}
\citation{ingber1989veryfastsimulatedreannealing}
\citation{lecchinivisintini2007sacontinuousgaruntees}
\citation{cybenko1989approxsuperposition}
\citation{laarhoven1987satheoryapplication}
\citation{geman1984stochasticrelaxation}
\citation{kirkpatrick1983}
\citation{metropolis1953}
\citation{goffe1994globaloptimization}
\citation{corana1987minimizingmultimodal}
\citation{lecchinivisintini2007sacontinuousgaruntees}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:introduction}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review}{1}}
\newlabel{scn:lit_review}{{2}{1}}
\citation{szu1987fastsimulatedannealing}
\citation{tsallis1996generalizedsimulatedannealing}
\citation{engel1988teachingfeedforwardnueralnetsbysa}
\citation{lee2007improvinggeneralizationcapabilitynnusingsa}
\citation{lee2007improvinggeneralizationcapabilitynnusingsa}
\citation{lee2007improvinggeneralizationcapabilitynnusingsa}
\citation{sexton1999beyondbackpropusingsa}
\citation{mukherjee2015multivariatesearchqa}
\citation{roland2002quatumsearch}
\citation{das2005qakcs}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Formulation and Overview}{2}}
\newlabel{scn:problem_formulation}{{3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Representation}{2}}
\newlabel{scn:weight_selection}{{3.1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  (a) An arbitrary feed-forward ANN. (b) The weight matrix representation of the feed-forward ANN in (a).}}{2}}
\newlabel{fig:nettomatmapping}{{1}{2}}
\citation{mukherjee2015multivariatesearchqa}
\citation{szu1987fastsimulatedannealing}
\newlabel{fig:abstract_matrix_rep}{{3.1}{3}}
\newlabel{eq:cost_mapping}{{3.1}{3}}
\newlabel{eq:optimal_definition}{{3.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Neighborhood Functions}{3}}
\newlabel{scn:neighborhood_fuctions}{{3.2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Physical Interpretation of Neighborhood Functions}{3}}
\newlabel{scn:physical_interpretation_simulated_annealing}{{3.2.1}{3}}
\newlabel{eq:thermal_transistion_prob}{{1}{3}}
\newlabel{eq:quantum_transition_prob}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (Bottom) A simple step potential in one dimension. (Top) The probability density function of a generic quantum system in the presence of a potential energy step barrier. There is an exponential decrease in probability through the barrier, and a uniform probability beyond the barrier.}}{3}}
\newlabel{fig:quantum_tunneling}{{2}{3}}
\citation{tsallis1996generalizedsimulatedannealing}
\citation{mukherjee2015multivariatesearchqa}
\citation{szu1987fastsimulatedannealing}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  (a) A potential energy barrier on a one-dimensional potential energy surface. (b) The tunneling probability relative to the probability of traversal due to thermal fluctuation for a step barrier plotted as a function of the height and width of the barrier.}}{4}}
\newlabel{fig:quantum_advantage}{{3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Classical Neighborhood Functions}{4}}
\newlabel{scn:classical_neighborhood}{{3.2.2}{4}}
\newlabel{eq:classical_anisotropic_neighborhood}{{3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Quantum-Inspired Neighborhood Functions}{4}}
\newlabel{scn:quantum_neighborhood}{{3.2.3}{4}}
\newlabel{eq:quantum_anisotropic_neighborhood}{{4}{4}}
\newlabel{eq:exp_generation_fcn}{{5}{4}}
\newlabel{eq:cauchy_generation_fcn}{{6}{5}}
\newlabel{eq:uniform_generation_fcn}{{7}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Anisotropicity}{5}}
\newlabel{scn:anisotropicity}{{3.2.4}{5}}
\newlabel{eq:isotropic_anisotropicity}{{8}{5}}
\newlabel{eq:isotropic_anisotropicity_scaled}{{9}{5}}
\newlabel{eq:uniform_anisotropicity}{{11}{5}}
\citation{geman1984stochasticrelaxation}
\citation{szu1987fastsimulatedannealing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Cost Function}{6}}
\newlabel{scn:cost_functions}{{3.3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Annealing Schedules}{6}}
\newlabel{scn:annealing_schedules}{{3.4}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Classical Simulated Annealing}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Fast Simulated Annealing}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Synaptic Annealing Algorithm}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Synaptic Annealing}}{6}}
\newlabel{alg:synaptic annealing}{{1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments and Results}{6}}
\newlabel{scn:experiments_results}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Classification Performance}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Visiting Distribution Analysis}{6}}
\newlabel{block1a}{{1a}{7}}
\newlabel{block1b}{{1b}{7}}
\newlabel{block1c}{{1c}{7}}
\newlabel{block1d}{{1d}{7}}
\newlabel{block2a}{{2a}{7}}
\newlabel{block1b}{{2b}{7}}
\newlabel{block1b}{{2c}{7}}
\newlabel{block2b}{{2d}{7}}
\newlabel{block3a}{{3a}{7}}
\newlabel{block1b}{{3b}{7}}
\newlabel{block1b}{{3c}{7}}
\newlabel{block3b}{{3d}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The simulation-time evolution of the 25-fold cross-validated classification error of a feed-forward neural network with 50 hidden units on Fishers iris data. Each column in this table depicts the results for an SA neighborhood function employing a single type of visiting distribution, which is indicated in the column header. Each row depicts a single type of anisotropicity strategy, which is indicated in the row header. The classification error is defined as the fraction of samples incorrectly classified.}}{7}}
\newlabel{fig:class_perf}{{4}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Anisotropicity Analysis}{7}}
\newlabel{scn:anisotropicity_analysis}{{4.1.2}{7}}
\citation{ingber1989veryfastsimulatedreannealing}
\bibstyle{IEEEtran}
\bibdata{synapticAnnealingBib}
\bibcite{geman1984stochasticrelaxation}{1}
\bibcite{ingber1989veryfastsimulatedreannealing}{2}
\bibcite{lecchinivisintini2007sacontinuousgaruntees}{3}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Perfect Classification Rate Analysis}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all Gaussian visiting distributions, and thus corresponds to the first column of Fig.\nobreakspace  {}4.1.2\hbox {}}}{8}}
\newlabel{fig:gaussian_gamma}{{6}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all exponential visiting distributions, and thus corresponds to the second column of Fig.\nobreakspace  {}4.1.2\hbox {}}}{8}}
\newlabel{fig:exp_gamma}{{7}{8}}
\newlabel{scn:conclusion}{{5}{8}}
\@writefile{toc}{\contentsline {section}{References}{8}}
\newlabel{fig:perfect_class_time}{{4.1.2}{9}}
\newlabel{block1a}{{1a}{9}}
\newlabel{block1b}{{1b}{9}}
\newlabel{block1c}{{1c}{9}}
\newlabel{block1d}{{1d}{9}}
\newlabel{block2a}{{2a}{9}}
\newlabel{block1b}{{2b}{9}}
\newlabel{block1b}{{2c}{9}}
\newlabel{block2b}{{2d}{9}}
\newlabel{block3a}{{3a}{9}}
\newlabel{block1b}{{3b}{9}}
\newlabel{block1b}{{3c}{9}}
\newlabel{block3b}{{3d}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The simulation-time evolution of the fraction of 25 independent simulated annealing runs achieving perfect classification of Fishers iris data using a feed-forward neural network with 50 hidden units. Columns and rows are labeld as in Fig.\nobreakspace  {}4\hbox {}.}}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all uniform visiting distributions, and thus corresponds to the third column of Fig.\nobreakspace  {}4.1.2\hbox {}}}{9}}
\newlabel{fig:uniform_gamma}{{8}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces This figure shows the gamma distribution approximations of the probability of achieving perfect classification through simulation time. This figure depicts the approximations for all Cauchy visiting distributions, and thus corresponds to the fourth column of Fig.\nobreakspace  {}4.1.2\hbox {}}}{9}}
\newlabel{fig:cauchy_gamma}{{9}{9}}
\bibcite{cybenko1989approxsuperposition}{4}
\bibcite{laarhoven1987satheoryapplication}{5}
\bibcite{kirkpatrick1983}{6}
\bibcite{metropolis1953}{7}
\bibcite{goffe1994globaloptimization}{8}
\bibcite{corana1987minimizingmultimodal}{9}
\bibcite{szu1987fastsimulatedannealing}{10}
\bibcite{tsallis1996generalizedsimulatedannealing}{11}
\bibcite{engel1988teachingfeedforwardnueralnetsbysa}{12}
\bibcite{lee2007improvinggeneralizationcapabilitynnusingsa}{13}
\bibcite{sexton1999beyondbackpropusingsa}{14}
\bibcite{mukherjee2015multivariatesearchqa}{15}
\bibcite{roland2002quatumsearch}{16}
\bibcite{das2005qakcs}{17}
\@writefile{toc}{\contentsline {section}{Biographies}{10}}
\@writefile{toc}{\contentsline {subsection}{Justin R. Fletcher}{10}}
\@writefile{toc}{\contentsline {subsection}{Michael J. Mendenhall}{10}}
